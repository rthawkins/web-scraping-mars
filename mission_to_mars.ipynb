{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T14:50:48.252852Z",
     "start_time": "2019-11-09T14:50:48.247852Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "from splinter import Browser\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA Mars News\n",
    "Scrape the NASA Mars News Site and collect the latest News Title and Paragraph Text. Assign the text to variables that you can reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T00:47:24.387720Z",
     "start_time": "2019-11-08T00:47:13.873550Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare scraping via browser\n",
    "executable_path = {'executable_path': 'c:/chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# URL of page to be scraped\n",
    "url = 'https://mars.nasa.gov/news/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T00:47:27.505951Z",
     "start_time": "2019-11-08T00:47:27.232931Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve page via browser\n",
    "html = browser.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T00:47:30.308156Z",
     "start_time": "2019-11-08T00:47:30.105142Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(browser.html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T00:47:31.877272Z",
     "start_time": "2019-11-08T00:47:31.746265Z"
    }
   },
   "outputs": [],
   "source": [
    "# Examine the results, then determine element that contains sought info\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T00:47:37.836654Z",
     "start_time": "2019-11-08T00:47:37.818653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store data in variables\n",
    "title = soup.find(\"div\", class_=\"content_title\").text\n",
    "paragraph_text = soup.find(\"div\", class_=\"rollover_description_inner\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T00:47:39.934207Z",
     "start_time": "2019-11-08T00:47:39.928207Z"
    }
   },
   "outputs": [],
   "source": [
    "# Confirm stored data is correct\n",
    "print(f\"Title: {title}\")\n",
    "print(f\"Text: {paragraph_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JPL Mars Space Images - Featured Image\n",
    "* Visit the url for JPL Featured Space Image here: https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\n",
    "* Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called featured_image_url.\n",
    "* Make sure to find the image url to the full size .jpg image.\n",
    "* Make sure to save a complete url string for this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T00:49:02.959425Z",
     "start_time": "2019-11-08T00:48:54.297990Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run Chrome Driver\n",
    "executable_path = {'executable_path': 'c:/chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "url2 = \"https://jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "browser.visit(url2)\n",
    "\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(browser.html, 'html.parser')\n",
    "\n",
    "# Replace parts of the style element that aren't the URL\n",
    "image = soup.find(\"article\", class_=\"carousel_item\")[\"style\"]\n",
    "image = image.replace(\"background-image: url('\",\"\")\n",
    "image = image.replace(\"');\",\"\")\n",
    "\n",
    "# Store image path in variable\n",
    "img_url = \"https://jpl.nasa.gov\"+image\n",
    "\n",
    "featured_image_url = img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T00:49:09.784834Z",
     "start_time": "2019-11-08T00:49:09.764854Z"
    }
   },
   "outputs": [],
   "source": [
    "# Confirm stored link is correct\n",
    "featured_image_url "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Weather\n",
    "\n",
    "Visit the Mars Weather twitter account here and scrape the latest Mars weather tweet from the page. Save the tweet text for the weather report as a variable called mars_weather. https://twitter.com/marswxreport?lang=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T00:51:21.463957Z",
     "start_time": "2019-11-08T00:51:20.564237Z"
    }
   },
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url3 = 'https://twitter.com/marswxreport?lang=en/'\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url3)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T00:52:38.588197Z",
     "start_time": "2019-11-08T00:52:38.568087Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pull text from first tweet\n",
    "mars_weather = soup.find(class_=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\").text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T00:52:47.164161Z",
     "start_time": "2019-11-08T00:52:47.157161Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preview and confirm\n",
    "print(mars_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Facts\n",
    "\n",
    "\n",
    "* Visit the Mars Facts webpage here and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc. https://space-facts.com/mars/\n",
    "* Use Pandas to convert the data to a HTML table string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T00:53:44.613625Z",
     "start_time": "2019-11-08T00:53:44.609348Z"
    }
   },
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url4 = 'https://space-facts.com/mars/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T00:53:46.942550Z",
     "start_time": "2019-11-08T00:53:46.287578Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select target table\n",
    "facts_df = pd.read_html(url4)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T00:54:01.722485Z",
     "start_time": "2019-11-08T00:54:01.708508Z"
    }
   },
   "outputs": [],
   "source": [
    "# Confirm data is correct\n",
    "print(facts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T00:56:46.917378Z",
     "start_time": "2019-11-08T00:56:46.894374Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tranform to html and store in variable\n",
    "facts = facts_df.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T00:56:47.713438Z",
     "start_time": "2019-11-08T00:56:47.706440Z"
    }
   },
   "outputs": [],
   "source": [
    "print(facts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Hemispheres\n",
    "\n",
    "\n",
    "* Visit the USGS Astrogeology site here to obtain high resolution images for each of Mar's hemispheres. https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\n",
    "* You will need to click each of the links to the hemispheres in order to find the image url to the full resolution image.\n",
    "* Save both the image url string for the full resolution hemisphere image, and the Hemisphere title containing the hemisphere name. Use a Python dictionary to store the data using the keys img_url and title.\n",
    "* Append the dictionary with the image url string and the hemisphere title to a list. This list will contain one dictionary for each hemisphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T14:51:03.280489Z",
     "start_time": "2019-11-09T14:50:52.706838Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run Chrome Driver to target URL\n",
    "executable_path = {'executable_path': 'c:/chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "url3 = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.visit(url3)\n",
    "\n",
    "soup = BeautifulSoup(browser.html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T14:52:13.816034Z",
     "start_time": "2019-11-09T14:52:13.806825Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pull URLs for each option on page\n",
    "hemi_urls = []\n",
    "base_url = \"https://astrogeology.usgs.gov\"\n",
    "\n",
    "for div in soup.find(\"div\", class_=\"full-content\").find_all(\"div\", class_=\"item\"):\n",
    "    hemi_urls.append(base_url + div.a[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T14:52:20.607295Z",
     "start_time": "2019-11-09T14:52:20.598295Z"
    }
   },
   "outputs": [],
   "source": [
    "hemi_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T14:54:23.328946Z",
     "start_time": "2019-11-09T14:54:20.621901Z"
    }
   },
   "outputs": [],
   "source": [
    "# Return image name and URL for each\n",
    "hemi_list = []\n",
    "for link in hemi_urls:\n",
    "    browser.visit(link)\n",
    "    soup = BeautifulSoup(browser.html, \"html.parser\")\n",
    "    hemi_name = soup.find(\"h2\", class_=\"title\").text\n",
    "    hemi_image = soup.find(\"div\", class_=\"downloads\").find_all(\"li\")[1].a[\"href\"] + \"/full.jpg\"\n",
    "    hemi_dict = {\"img_url\": hemi_image,\n",
    "                       \"title\": hemi_name}\n",
    "    hemi_list.append(hemi_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T14:54:24.801126Z",
     "start_time": "2019-11-09T14:54:24.794126Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preview and confirm data\n",
    "hemi_list"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
